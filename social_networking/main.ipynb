{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d383017",
   "metadata": {},
   "source": [
    "### Pho bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c5684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinhbq/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.2\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "Dtype dùng: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import & cấu hình thiết bị (GPU/FP16)\n",
    "\n",
    "import os, sys, gc, math, pathlib, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Thiết bị & dtype (FP16 nếu có GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "dtype = torch.float16 if use_fp16 else torch.float32\n",
    "print(\"Dtype dùng:\", dtype)\n",
    "\n",
    "# Thư mục output\n",
    "OUT_DIR = \"outputs_phobert\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78451364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cột: ['category_name', 'description']\n",
      "Tổng dòng gốc: 36644\n",
      "NaN: 0 | empty '': 0 | whitespace-only: 0\n",
      "Số dòng trùng mô tả (không tính dòng đầu tiên): 3429\n",
      "\n",
      "Số dòng sau khi loại trắng: 36644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/description_phobert(1).csv\"  \n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Cột:\", df.columns.tolist())\n",
    "print(\"Tổng dòng gốc:\", len(df))\n",
    "\n",
    "df[\"category_name\"] = df[\"category_name\"].fillna(\"unknown\").astype(str)\n",
    "df[\"description\"]   = df[\"description\"].fillna(\"\").astype(str)\n",
    "\n",
    "n_na        = df[\"description\"].isna().sum()\n",
    "n_empty     = (df[\"description\"] == \"\").sum()\n",
    "n_ws_only   = df[\"description\"].str.strip().eq(\"\").sum()\n",
    "\n",
    "print(f\"NaN: {n_na} | empty '': {n_empty} | whitespace-only: {n_ws_only}\")\n",
    "\n",
    "n_dups_desc = df.duplicated(subset=[\"description\"]).sum()\n",
    "print(f\"Số dòng trùng mô tả (không tính dòng đầu tiên): {n_dups_desc}\")\n",
    "\n",
    "ex_ws = df[df[\"description\"].str.strip().eq(\"\")].head(5)\n",
    "if len(ex_ws):\n",
    "    print(\"\\nVí dụ mô tả trắng/rỗng:\")\n",
    "    display(ex_ws)\n",
    "\n",
    "df_keep = df[df[\"description\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "print(\"\\nSố dòng sau khi loại trắng:\", len(df_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea365df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda | dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"vinai/phobert-large\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model = model.to(device=device, dtype=dtype)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9bd38",
   "metadata": {},
   "source": [
    "### word-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181b2faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-11 17:55:12 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
      "VnCoreNLP loaded.\n",
      "Ví dụ sau khi word-seg:\n",
      "                                         description  \\\n",
      "0  loa loa sản phẩm nguyên sản phẩm nguyên cam kế...   \n",
      "1  đặc điểm nổi bật đặc điểm nổi bật công nghệ ch...   \n",
      "2  thông số kỹ thuật loa 3 loa di động thương hiệ...   \n",
      "\n",
      "                                    description_wseg  \n",
      "0  loa loa sản_phẩm nguyên sản_phẩm nguyên cam_kế...  \n",
      "1  đặc_điểm nổi_bật đặc_điểm nổi_bật công_nghệ ch...  \n",
      "2  thông_số kỹ_thuật loa 3 loa di_động thương_hiệ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import py_vncorenlp\n",
    "\n",
    "VNCORE_DIR = \"/home/thinhbq/huy/social_networking/pho_bert/vncorenlp\"\n",
    "os.makedirs(VNCORE_DIR, exist_ok=True)\n",
    "# py_vncorenlp.download_model(save_dir=VNCORE_DIR)\n",
    "\n",
    "if \"rdrsegmenter\" not in globals():\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=VNCORE_DIR)\n",
    "    print(\"VnCoreNLP loaded.\")\n",
    "else:\n",
    "    print(\"Reusing existing rdrsegmenter (already loaded).\")\n",
    "\n",
    "def vn_word_segment(text: str) -> str:\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    sents = rdrsegmenter.word_segment(text)\n",
    "    return \" \".join(sents)\n",
    "\n",
    "df_keep[\"description_wseg\"] = [vn_word_segment(t) for t in df_keep[\"description\"]]\n",
    "\n",
    "print(\"Ví dụ sau khi word-seg:\")\n",
    "print(df_keep[[\"description\", \"description_wseg\"]].head(3))\n",
    "\n",
    "df_keep.to_parquet(\"description_phobert_wseg.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251729c",
   "metadata": {},
   "source": [
    "### embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d38705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 573/573 [04:55<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from contextlib import nullcontext\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DATA_PATH = \"description_phobert_wseg.parquet\"\n",
    "df_wseg = pd.read_parquet(DATA_PATH)\n",
    "texts = df_wseg[\"description_wseg\"].tolist()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts): self.texts = texts\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx): return self.texts[idx]\n",
    "\n",
    "dataset = TextDataset(texts)\n",
    "\n",
    "def collate_batch(batch_texts):\n",
    "    return tokenizer(\n",
    "        batch_texts,\n",
    "        padding=\"max_length\",      \n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=2 if device.type == \"cuda\" else 0,\n",
    "    pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state      \n",
    "    mask = attention_mask.unsqueeze(-1).expand_as(token_embeddings).float()\n",
    "    return (token_embeddings * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "\n",
    "all_embeddings = []\n",
    "model.eval()\n",
    "\n",
    "amp_ctx = torch.autocast(device_type=\"cuda\", dtype=dtype) if device.type==\"cuda\" else nullcontext()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(loader, desc=\"Encoding\"):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        with amp_ctx:\n",
    "            outputs = model(**batch)\n",
    "            emb = mean_pooling(outputs, batch[\"attention_mask\"])\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1) \n",
    "        all_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "assert embeddings.shape[0] == len(df_wseg), \"Số embedding không khớp số dòng metadata!\"\n",
    "\n",
    "np.save(\"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\", embeddings)\n",
    "df_wseg.to_parquet(\"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce946bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (36644, 1024)\n",
      "Meta: (36644, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR   = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert\"\n",
    "EMB_PATH  = f\"{OUT_DIR}/phobert_embeddings.npy\"\n",
    "META_PATH = f\"{OUT_DIR}/metadata_phobert.parquet\"\n",
    "\n",
    "assert Path(EMB_PATH).exists(), EMB_PATH\n",
    "assert Path(META_PATH).exists(), META_PATH\n",
    "\n",
    "embeddings = np.load(EMB_PATH)                 \n",
    "meta       = pd.read_parquet(META_PATH)        \n",
    "assert embeddings.shape[0] == len(meta)\n",
    "\n",
    "print(\"Embeddings:\", embeddings.shape)\n",
    "print(\"Meta:\", meta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0730559",
   "metadata": {},
   "source": [
    "### Kmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611c9a1",
   "metadata": {},
   "source": [
    "#### Grid search tìm K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2ccd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K  silhouette_cosine\n",
      "12           0.204663\n",
      "16           0.194674\n",
      "20           0.192583\n",
      " 4           0.190894\n",
      " 8           0.177111\n",
      "30           0.175496\n",
      "28           0.175351\n",
      "24           0.173827\n",
      "40           0.166742\n",
      "50           0.160448\n",
      "best_K: 12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "EMB_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
    "Ks = [4, 8, 12, 16, 20, 24, 28, 30, 40, 50]\n",
    "\n",
    "X  = np.load(EMB_PATH)\n",
    "Xn = normalize(X)\n",
    "\n",
    "rows = []\n",
    "for K in Ks:\n",
    "    labels = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit_predict(Xn)\n",
    "    sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
    "    rows.append((K, sil))\n",
    "\n",
    "df_k = pd.DataFrame(rows, columns=[\"K\",\"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
    "best_K = int(df_k.iloc[0][\"K\"])\n",
    "print(df_k.to_string(index=False))\n",
    "print(\"best_K:\", best_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa512c0c",
   "metadata": {},
   "source": [
    "#### áp dụng Kmean với K tìm được"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9907adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_K: 12 | n_clusters: 12\n",
      "cluster 0: 1757\n",
      "cluster 1: 1418\n",
      "cluster 2: 2986\n",
      "cluster 3: 2504\n",
      "cluster 4: 3488\n",
      "cluster 5: 2251\n",
      "cluster 6: 4104\n",
      "cluster 7: 3327\n",
      "cluster 8: 4298\n",
      "cluster 9: 1842\n",
      "cluster 10: 2939\n",
      "cluster 11: 5730\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "km = KMeans(n_clusters=best_K, n_init=\"auto\", random_state=42).fit(Xn)\n",
    "labels = km.labels_\n",
    "\n",
    "uniq, cnt = np.unique(labels, return_counts=True)\n",
    "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
    "for k, c in zip(uniq, cnt):\n",
    "    print(f\"cluster {k}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c31b0",
   "metadata": {},
   "source": [
    "#### đánh giá trong trường hợp không có nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4942726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unsupervised] Silhouette(cosine)=0.2047 | DBI=2.2994 | CH=2418.46\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import os\n",
    "\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
    "dbi = float(davies_bouldin_score(Xn, labels))\n",
    "ch  = float(calinski_harabasz_score(Xn, labels))\n",
    "\n",
    "print(f\"[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"KMeans\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": best_K,\n",
    "    \"type\": \"unsupervised\",\n",
    "    \"silhouette_cosine\": sil,\n",
    "    \"DBI\": dbi,\n",
    "    \"CH\": ch,\n",
    "    \"NMI\": None,\n",
    "    \"ARI\": None,\n",
    "    \"Purity\": None\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057dba5",
   "metadata": {},
   "source": [
    "#### đánh giá trong trường hợp có nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00440e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Supervised] NMI=0.5477 | ARI=0.3781 | Purity=0.5329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2186194/2530096801.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_out = pd.concat([old, record], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "import os\n",
    "\n",
    "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "df_meta = pd.read_parquet(META_PATH)\n",
    "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    return cm.max(axis=0).sum() / cm.sum()\n",
    "\n",
    "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
    "ari = float(adjusted_rand_score(y_true, labels))\n",
    "pur = float(purity_score(y_true, labels))\n",
    "\n",
    "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"KMeans\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": best_K,\n",
    "    \"type\": \"supervised\",\n",
    "    \"silhouette_cosine\": None,\n",
    "    \"DBI\": None,\n",
    "    \"CH\": None,\n",
    "    \"NMI\": nmi,\n",
    "    \"ARI\": ari,\n",
    "    \"Purity\": pur\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc6dbb",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcb084",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160ad96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eps  min_samples  n_clusters  noise_frac  silhouette_cosine      DBI        CH\n",
      "0.007           15           2    0.020358           0.555812 0.837081 48.361832\n",
      "0.007           12           2    0.019348           0.555514 0.837299 48.325653\n",
      "0.007           10           2    0.018912           0.555455 0.837356 48.317505\n",
      "0.007            8           2    0.018339           0.555242 0.837513 48.293240\n",
      "0.013            5           3    0.000655           0.526894 1.023394 38.960388\n",
      "0.013            8           2    0.000791           0.526874 1.194343 55.853313\n",
      "0.013           10           2    0.000791           0.526874 1.194343 55.853313\n",
      "0.013           12           2    0.000928           0.514477 1.179943 50.203739\n",
      "0.013           15           2    0.000928           0.514477 1.179943 50.203739\n",
      "0.009           10           2    0.004557           0.470796 1.123968 33.887119\n",
      "0.009            8           2    0.004475           0.470770 1.123992 33.884640\n",
      "0.011            8           2    0.001610           0.469658 1.124797 33.782219\n",
      "0.011           10           2    0.001610           0.469658 1.124797 33.782219\n",
      "0.011            5           3    0.001446           0.469643 0.928655 28.821766\n",
      "0.009            5           4    0.003930           0.467762 0.884960 28.377533\n",
      "\n",
      " Chọn: eps=0.007, min_samples=15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "EMB_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
    "\n",
    "EPS_COARSE = [0.003, 0.005, 0.007, 0.009, 0.011, 0.013]\n",
    "MS_LIST = [5, 8, 10, 12, 15]\n",
    "\n",
    "X  = np.load(EMB_PATH)\n",
    "Xn = normalize(X)\n",
    "\n",
    "def eval_dbscan(eps_list, ms_list):\n",
    "    rows = []\n",
    "    for eps in eps_list:\n",
    "        for ms in ms_list:\n",
    "            labels = DBSCAN(eps=eps, min_samples=ms, metric=\"cosine\", n_jobs=-1).fit_predict(Xn)\n",
    "            mask = labels != -1\n",
    "            n_noise = int((~mask).sum())\n",
    "            n_keep  = int(mask.sum())\n",
    "            n_clu   = len(np.unique(labels[mask])) if n_keep > 0 else 0\n",
    "            if n_clu >= 2:\n",
    "                sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
    "                dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
    "                ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
    "            else:\n",
    "                sil = dbi = ch = np.nan\n",
    "            rows.append({\n",
    "                \"eps\": float(eps), \"min_samples\": int(ms),\n",
    "                \"n_clusters\": int(n_clu),\n",
    "                \"noise_frac\": n_noise / Xn.shape[0],\n",
    "                \"silhouette_cosine\": sil, \"DBI\": dbi, \"CH\": ch\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df1 = eval_dbscan(EPS_COARSE, MS_LIST)\n",
    "_df1 = df1.copy()\n",
    "_df1[\"_DBI_sort\"] = _df1[\"DBI\"].fillna(np.inf)\n",
    "_df1 = _df1.sort_values(\n",
    "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
    "    ascending=[False, True, False, True]\n",
    ")\n",
    "top1 = _df1.iloc[0]\n",
    "best_eps_coarse = float(top1[\"eps\"])\n",
    "\n",
    "ref_lo = max(0.001, best_eps_coarse - 0.002)\n",
    "ref_hi = best_eps_coarse + 0.002\n",
    "EPS_FINE = np.round(np.arange(ref_lo, ref_hi + 1e-9, 0.001), 3)\n",
    "\n",
    "\n",
    "df2 = eval_dbscan(EPS_FINE, MS_LIST)\n",
    "df_all = pd.concat([df1, df2], ignore_index=True).drop_duplicates(subset=[\"eps\",\"min_samples\"])\n",
    "\n",
    "_df = df_all.copy()\n",
    "_df[\"_DBI_sort\"] = _df[\"DBI\"].fillna(np.inf)\n",
    "_df = _df.sort_values(\n",
    "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
    "    ascending=[False, True, False, True]\n",
    ")\n",
    "\n",
    "best = _df.iloc[0]\n",
    "best_eps = float(best[\"eps\"])\n",
    "best_ms  = int(best[\"min_samples\"])\n",
    "\n",
    "print(_df.drop(columns=[\"_DBI_sort\"]).head(15).to_string(index=False))\n",
    "print(f\"\\n Chọn: eps={best_eps}, min_samples={best_ms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0a235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=0.005 → n_clusters=17, noise=11.2%\n",
      "eps=0.010 → n_clusters=2, noise=0.3%\n",
      "eps=0.015 → n_clusters=1, noise=0.1%\n",
      "eps=0.020 → n_clusters=1, noise=0.0%\n",
      "eps=0.025 → n_clusters=1, noise=0.0%\n",
      "eps=0.030 → n_clusters=1, noise=0.0%\n",
      "eps=0.035 → n_clusters=1, noise=0.0%\n",
      "eps=0.040 → n_clusters=1, noise=0.0%\n",
      "eps=0.045 → n_clusters=1, noise=0.0%\n"
     ]
    }
   ],
   "source": [
    "for eps in np.arange(0.005, 0.05, 0.005):\n",
    "    labels = DBSCAN(eps=eps, min_samples=10, metric='cosine', n_jobs=-1).fit_predict(Xn)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    noise_frac = (labels == -1).mean()\n",
    "    print(f\"eps={eps:.3f} → n_clusters={n_clusters}, noise={noise_frac:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7cefa",
   "metadata": {},
   "source": [
    "#### Áp dụng với DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_ms, metric=\"cosine\", n_jobs=-1).fit(Xn)\n",
    "labels = db.labels_\n",
    "\n",
    "noise = int((labels == -1).sum())\n",
    "uniq, cnt = np.unique(labels[labels != -1], return_counts=True)\n",
    "print(f\"eps={best_eps}, min_samples={best_ms}\")\n",
    "print(f\"clusters={len(uniq)} | noise={noise} / {len(labels)} ({noise/len(labels):.2%})\")\n",
    "for k, c in zip(uniq, cnt):\n",
    "    print(f\"cluster {k}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc59ce",
   "metadata": {},
   "source": [
    "#### đánh giá ( trong trường hợp không biết nhãn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN (COSINE) — Cell 3: unsupervised eval + append CSV\n",
    "import pandas as pd, os\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "mask = (labels != -1)\n",
    "if mask.sum() >= 2 and len(np.unique(labels[mask])) >= 2:\n",
    "    sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
    "    dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
    "    ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
    "else:\n",
    "    sil = dbi = ch = float(\"nan\")\n",
    "\n",
    "print(f\"[DBSCAN-cosine | unsupervised] sil={sil:.4f} | DBI={dbi} | CH={ch}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"DBSCAN\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": \"\",            \n",
    "    \"type\": \"unsupervised\",\n",
    "    \"silhouette_cosine\": sil,\n",
    "    \"DBI\": dbi,\n",
    "    \"CH\": ch,\n",
    "    \"NMI\": None,\n",
    "    \"ARI\": None,\n",
    "    \"Purity\": None\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ba9e4",
   "metadata": {},
   "source": [
    "#### đánh giá ( trong trường hợp biết nhãn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "df_meta = pd.read_parquet(META_PATH)\n",
    "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    return cm.max(axis=0).sum() / cm.sum()\n",
    "\n",
    "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
    "ari = float(adjusted_rand_score(y_true, labels))\n",
    "pur = float(purity_score(y_true, labels))\n",
    "\n",
    "print(f\"[DBSCAN-cosine | supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"DBSCAN\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": \"\",\n",
    "    \"type\": \"supervised\",\n",
    "    \"silhouette_cosine\": None,\n",
    "    \"DBI\": None,\n",
    "    \"CH\": None,\n",
    "    \"NMI\": nmi,\n",
    "    \"ARI\": ari,\n",
    "    \"Purity\": pur\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371f4ee",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89744e",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4e9b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K  silhouette_cosine\n",
      " 4           0.263736\n",
      "12           0.205098\n",
      "28           0.199713\n",
      "20           0.196263\n",
      "16           0.195344\n",
      "30           0.193094\n",
      "14           0.183494\n",
      " 8           0.179954\n",
      "24           0.179773\n",
      "best_K: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "EMB_PATH = \"/Users/huy/Documents/Hutech/HK1A 2025-2026/social-networking/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
    "Ks = [4, 8, 12, 14, 16, 20, 24, 28, 30]\n",
    "\n",
    "X = np.load(EMB_PATH).astype(np.float64)\n",
    "Xn = normalize(X)\n",
    "\n",
    "rows = []\n",
    "for K in Ks:\n",
    "    gmm = GaussianMixture(n_components=K, covariance_type=\"diag\", reg_covar=1e-4, random_state=42)\n",
    "    labels = gmm.fit_predict(Xn)\n",
    "    sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
    "    rows.append((K, sil))\n",
    "\n",
    "df_k = pd.DataFrame(rows, columns=[\"K\", \"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
    "best_K = int(df_k.iloc[0][\"K\"])\n",
    "print(df_k.to_string(index=False))\n",
    "print(\"best_K:\", best_K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceafbf3",
   "metadata": {},
   "source": [
    "#### Áp dụng với K tìm được "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42052d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_K: 4 | n_clusters: 4\n",
      "cluster 0: 13975\n",
      "cluster 1: 14175\n",
      "cluster 2: 4945\n",
      "cluster 3: 3549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "gmm = GaussianMixture(n_components=best_K, covariance_type=\"diag\", reg_covar=1e-4, random_state=42)\n",
    "labels = gmm.fit_predict(Xn)\n",
    "\n",
    "uniq, cnt = np.unique(labels, return_counts=True)\n",
    "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
    "for k, c in zip(uniq, cnt):\n",
    "    print(f\"cluster {k}: {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab31d2",
   "metadata": {},
   "source": [
    "#### đánh giá không nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
    "dbi = float(davies_bouldin_score(Xn, labels))\n",
    "ch = float(calinski_harabasz_score(Xn, labels))\n",
    "\n",
    "print(f\"[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"GMM\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": best_K,\n",
    "    \"type\": \"unsupervised\",\n",
    "    \"silhouette_cosine\": sil,\n",
    "    \"DBI\": dbi,\n",
    "    \"CH\": ch,\n",
    "    \"NMI\": None,\n",
    "    \"ARI\": None,\n",
    "    \"Purity\": None\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d8acf",
   "metadata": {},
   "source": [
    "#### đánh giá khi biết nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
    "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
    "\n",
    "df_meta = pd.read_parquet(META_PATH)\n",
    "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    return cm.max(axis=0).sum() / cm.sum()\n",
    "\n",
    "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
    "ari = float(adjusted_rand_score(y_true, labels))\n",
    "pur = float(purity_score(y_true, labels))\n",
    "\n",
    "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
    "\n",
    "record = pd.DataFrame([{\n",
    "    \"model\": \"GMM\",\n",
    "    \"metric\": \"cosine\",\n",
    "    \"K\": best_K,\n",
    "    \"type\": \"supervised\",\n",
    "    \"silhouette_cosine\": None,\n",
    "    \"DBI\": None,\n",
    "    \"CH\": None,\n",
    "    \"NMI\": nmi,\n",
    "    \"ARI\": ari,\n",
    "    \"Purity\": pur\n",
    "}])\n",
    "\n",
    "if os.path.exists(EVAL_PATH):\n",
    "    old = pd.read_csv(EVAL_PATH)\n",
    "    df_out = pd.concat([old, record], ignore_index=True)\n",
    "else:\n",
    "    df_out = record\n",
    "\n",
    "df_out.to_csv(EVAL_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f74a8e",
   "metadata": {},
   "source": [
    "## multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7a72b",
   "metadata": {},
   "source": [
    "### Kmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6b095",
   "metadata": {},
   "source": [
    "#### grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a4fd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
