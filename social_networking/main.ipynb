{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d383017",
      "metadata": {
        "id": "8d383017"
      },
      "source": [
        "### Pho bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c5684b",
      "metadata": {
        "id": "e1c5684b",
        "outputId": "a887c529-560b-499d-fa26-81b1b323b526"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thinhbq/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.2\n",
            "PyTorch: 2.8.0+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA L4\n",
            "Dtype dùng: torch.float16\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import & cấu hình thiết bị (GPU/FP16)\n",
        "\n",
        "import os, sys, gc, math, pathlib, shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Thiết bị & dtype (FP16 nếu có GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "dtype = torch.float16 if use_fp16 else torch.float32\n",
        "print(\"Dtype dùng:\", dtype)\n",
        "\n",
        "# Thư mục output\n",
        "OUT_DIR = \"outputs_phobert\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78451364",
      "metadata": {
        "id": "78451364",
        "outputId": "6c433e95-23a7-409e-c3d6-09efb1a5d5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cột: ['category_name', 'description']\n",
            "Tổng dòng gốc: 36644\n",
            "NaN: 0 | empty '': 0 | whitespace-only: 0\n",
            "Số dòng trùng mô tả (không tính dòng đầu tiên): 3429\n",
            "\n",
            "Số dòng sau khi loại trắng: 36644\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/description_phobert(1).csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(\"Cột:\", df.columns.tolist())\n",
        "print(\"Tổng dòng gốc:\", len(df))\n",
        "\n",
        "df[\"category_name\"] = df[\"category_name\"].fillna(\"unknown\").astype(str)\n",
        "df[\"description\"]   = df[\"description\"].fillna(\"\").astype(str)\n",
        "\n",
        "n_na        = df[\"description\"].isna().sum()\n",
        "n_empty     = (df[\"description\"] == \"\").sum()\n",
        "n_ws_only   = df[\"description\"].str.strip().eq(\"\").sum()\n",
        "\n",
        "print(f\"NaN: {n_na} | empty '': {n_empty} | whitespace-only: {n_ws_only}\")\n",
        "\n",
        "n_dups_desc = df.duplicated(subset=[\"description\"]).sum()\n",
        "print(f\"Số dòng trùng mô tả (không tính dòng đầu tiên): {n_dups_desc}\")\n",
        "\n",
        "ex_ws = df[df[\"description\"].str.strip().eq(\"\")].head(5)\n",
        "if len(ex_ws):\n",
        "    print(\"\\nVí dụ mô tả trắng/rỗng:\")\n",
        "    display(ex_ws)\n",
        "\n",
        "df_keep = df[df[\"description\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
        "print(\"\\nSố dòng sau khi loại trắng:\", len(df_keep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea365df3",
      "metadata": {
        "id": "ea365df3",
        "outputId": "daaaa602-113b-4922-e738-39ed8cca5a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda | dtype: torch.float16\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(64001, 1024, padding_idx=1)\n",
              "    (position_embeddings): Embedding(258, 1024, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "MODEL_NAME = \"vinai/phobert-large\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "print(\"device:\", device, \"| dtype:\", dtype)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device=device, dtype=dtype)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc9bd38",
      "metadata": {
        "id": "acc9bd38"
      },
      "source": [
        "### word-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181b2faf",
      "metadata": {
        "id": "181b2faf",
        "outputId": "ee8a029c-8249-42bc-b3e2-7fcc87b220fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-11 17:55:12 INFO  WordSegmenter:24 - Loading Word Segmentation model\n",
            "VnCoreNLP loaded.\n",
            "Ví dụ sau khi word-seg:\n",
            "                                         description  \\\n",
            "0  loa loa sản phẩm nguyên sản phẩm nguyên cam kế...   \n",
            "1  đặc điểm nổi bật đặc điểm nổi bật công nghệ ch...   \n",
            "2  thông số kỹ thuật loa 3 loa di động thương hiệ...   \n",
            "\n",
            "                                    description_wseg  \n",
            "0  loa loa sản_phẩm nguyên sản_phẩm nguyên cam_kế...  \n",
            "1  đặc_điểm nổi_bật đặc_điểm nổi_bật công_nghệ ch...  \n",
            "2  thông_số kỹ_thuật loa 3 loa di_động thương_hiệ...  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import py_vncorenlp\n",
        "\n",
        "VNCORE_DIR = \"/home/thinhbq/huy/social_networking/pho_bert/vncorenlp\"\n",
        "os.makedirs(VNCORE_DIR, exist_ok=True)\n",
        "# py_vncorenlp.download_model(save_dir=VNCORE_DIR)\n",
        "\n",
        "if \"rdrsegmenter\" not in globals():\n",
        "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=VNCORE_DIR)\n",
        "    print(\"VnCoreNLP loaded.\")\n",
        "else:\n",
        "    print(\"Reusing existing rdrsegmenter (already loaded).\")\n",
        "\n",
        "def vn_word_segment(text: str) -> str:\n",
        "    if not text.strip():\n",
        "        return \"\"\n",
        "    sents = rdrsegmenter.word_segment(text)\n",
        "    return \" \".join(sents)\n",
        "\n",
        "df_keep[\"description_wseg\"] = [vn_word_segment(t) for t in df_keep[\"description\"]]\n",
        "\n",
        "print(\"Ví dụ sau khi word-seg:\")\n",
        "print(df_keep[[\"description\", \"description_wseg\"]].head(3))\n",
        "\n",
        "df_keep.to_parquet(\"description_phobert_wseg.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6251729c",
      "metadata": {
        "id": "6251729c"
      },
      "source": [
        "### embeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d38705",
      "metadata": {
        "id": "e9d38705",
        "outputId": "85b30f6a-b578-4c3a-ad79-49b10c809781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding: 100%|██████████| 573/573 [04:55<00:00,  1.94it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "DATA_PATH = \"description_phobert_wseg.parquet\"\n",
        "df_wseg = pd.read_parquet(DATA_PATH)\n",
        "texts = df_wseg[\"description_wseg\"].tolist()\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts): self.texts = texts\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx): return self.texts[idx]\n",
        "\n",
        "dataset = TextDataset(texts)\n",
        "\n",
        "def collate_batch(batch_texts):\n",
        "    return tokenizer(\n",
        "        batch_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=2 if device.type == \"cuda\" else 0,\n",
        "    pin_memory=(device.type == \"cuda\")\n",
        ")\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output.last_hidden_state\n",
        "    mask = attention_mask.unsqueeze(-1).expand_as(token_embeddings).float()\n",
        "    return (token_embeddings * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "\n",
        "all_embeddings = []\n",
        "model.eval()\n",
        "\n",
        "amp_ctx = torch.autocast(device_type=\"cuda\", dtype=dtype) if device.type==\"cuda\" else nullcontext()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in tqdm(loader, desc=\"Encoding\"):\n",
        "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "        with amp_ctx:\n",
        "            outputs = model(**batch)\n",
        "            emb = mean_pooling(outputs, batch[\"attention_mask\"])\n",
        "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
        "        all_embeddings.append(emb.cpu().numpy())\n",
        "\n",
        "embeddings = np.vstack(all_embeddings)\n",
        "assert embeddings.shape[0] == len(df_wseg), \"Số embedding không khớp số dòng metadata!\"\n",
        "\n",
        "np.save(\"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\", embeddings)\n",
        "df_wseg.to_parquet(\"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce946bd",
      "metadata": {
        "id": "8ce946bd",
        "outputId": "89e08dc3-91e5-495d-9442-92cde4b4238d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings: (36644, 1024)\n",
            "Meta: (36644, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR   = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert\"\n",
        "EMB_PATH  = f\"{OUT_DIR}/phobert_embeddings.npy\"\n",
        "META_PATH = f\"{OUT_DIR}/metadata_phobert.parquet\"\n",
        "\n",
        "assert Path(EMB_PATH).exists(), EMB_PATH\n",
        "assert Path(META_PATH).exists(), META_PATH\n",
        "\n",
        "embeddings = np.load(EMB_PATH)\n",
        "meta       = pd.read_parquet(META_PATH)\n",
        "assert embeddings.shape[0] == len(meta)\n",
        "\n",
        "print(\"Embeddings:\", embeddings.shape)\n",
        "print(\"Meta:\", meta.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0730559",
      "metadata": {
        "id": "a0730559"
      },
      "source": [
        "### Kmean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9611c9a1",
      "metadata": {
        "id": "9611c9a1"
      },
      "source": [
        "#### Grid search tìm K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d2ccd45",
      "metadata": {
        "id": "3d2ccd45",
        "outputId": "4e193340-8974-4226-ed78-9e0b1c5a3acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " K  silhouette_cosine\n",
            "12           0.204663\n",
            "16           0.194674\n",
            "20           0.192583\n",
            " 4           0.190894\n",
            " 8           0.177111\n",
            "30           0.175496\n",
            "28           0.175351\n",
            "24           0.173827\n",
            "40           0.166742\n",
            "50           0.160448\n",
            "best_K: 12\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "EMB_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
        "Ks = [4, 8, 12, 16, 20, 24, 28, 30, 40, 50]\n",
        "\n",
        "X  = np.load(EMB_PATH)\n",
        "Xn = normalize(X)\n",
        "\n",
        "rows = []\n",
        "for K in Ks:\n",
        "    labels = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit_predict(Xn)\n",
        "    sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "    rows.append((K, sil))\n",
        "\n",
        "df_k = pd.DataFrame(rows, columns=[\"K\",\"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
        "best_K = int(df_k.iloc[0][\"K\"])\n",
        "print(df_k.to_string(index=False))\n",
        "print(\"best_K:\", best_K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa512c0c",
      "metadata": {
        "id": "aa512c0c"
      },
      "source": [
        "#### áp dụng Kmean với K tìm được"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9907adef",
      "metadata": {
        "id": "9907adef",
        "outputId": "0afec54d-4e0c-488e-cb73-041611ae46ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_K: 12 | n_clusters: 12\n",
            "cluster 0: 1757\n",
            "cluster 1: 1418\n",
            "cluster 2: 2986\n",
            "cluster 3: 2504\n",
            "cluster 4: 3488\n",
            "cluster 5: 2251\n",
            "cluster 6: 4104\n",
            "cluster 7: 3327\n",
            "cluster 8: 4298\n",
            "cluster 9: 1842\n",
            "cluster 10: 2939\n",
            "cluster 11: 5730\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "km = KMeans(n_clusters=best_K, n_init=\"auto\", random_state=42).fit(Xn)\n",
        "labels = km.labels_\n",
        "\n",
        "uniq, cnt = np.unique(labels, return_counts=True)\n",
        "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a9c31b0",
      "metadata": {
        "id": "5a9c31b0"
      },
      "source": [
        "#### đánh giá trong trường hợp không có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4942726",
      "metadata": {
        "id": "f4942726",
        "outputId": "cb8414cd-8ee8-45ae-a3c7-522738b555fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Unsupervised] Silhouette(cosine)=0.2047 | DBI=2.2994 | CH=2418.46\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import os\n",
        "\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "dbi = float(davies_bouldin_score(Xn, labels))\n",
        "ch  = float(calinski_harabasz_score(Xn, labels))\n",
        "\n",
        "print(f\"[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"KMeans\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8057dba5",
      "metadata": {
        "id": "8057dba5"
      },
      "source": [
        "#### đánh giá trong trường hợp có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00440e29",
      "metadata": {
        "id": "00440e29",
        "outputId": "0e9a6636-7c4a-4343-d80e-20b038757812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised] NMI=0.5477 | ARI=0.3781 | Purity=0.5329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2186194/2530096801.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "import os\n",
        "\n",
        "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "df_meta = pd.read_parquet(META_PATH)\n",
        "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
        "ari = float(adjusted_rand_score(y_true, labels))\n",
        "pur = float(purity_score(y_true, labels))\n",
        "\n",
        "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"KMeans\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5dc6dbb",
      "metadata": {
        "id": "e5dc6dbb"
      },
      "source": [
        "### DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbbcb084",
      "metadata": {
        "id": "cbbcb084"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160ad96b",
      "metadata": {
        "id": "160ad96b",
        "outputId": "30ef447d-0d38-4ee5-c9ce-c0026f8e6d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  eps  min_samples  n_clusters  noise_frac  silhouette_cosine      DBI        CH\n",
            "0.007           15           2    0.020358           0.555812 0.837081 48.361832\n",
            "0.007           12           2    0.019348           0.555514 0.837299 48.325653\n",
            "0.007           10           2    0.018912           0.555455 0.837356 48.317505\n",
            "0.007            8           2    0.018339           0.555242 0.837513 48.293240\n",
            "0.013            5           3    0.000655           0.526894 1.023394 38.960388\n",
            "0.013            8           2    0.000791           0.526874 1.194343 55.853313\n",
            "0.013           10           2    0.000791           0.526874 1.194343 55.853313\n",
            "0.013           12           2    0.000928           0.514477 1.179943 50.203739\n",
            "0.013           15           2    0.000928           0.514477 1.179943 50.203739\n",
            "0.009           10           2    0.004557           0.470796 1.123968 33.887119\n",
            "0.009            8           2    0.004475           0.470770 1.123992 33.884640\n",
            "0.011            8           2    0.001610           0.469658 1.124797 33.782219\n",
            "0.011           10           2    0.001610           0.469658 1.124797 33.782219\n",
            "0.011            5           3    0.001446           0.469643 0.928655 28.821766\n",
            "0.009            5           4    0.003930           0.467762 0.884960 28.377533\n",
            "\n",
            " Chọn: eps=0.007, min_samples=15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "EMB_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
        "\n",
        "EPS_COARSE = [0.003, 0.005, 0.007, 0.009, 0.011, 0.013]\n",
        "MS_LIST = [5, 8, 10, 12, 15]\n",
        "\n",
        "X  = np.load(EMB_PATH)\n",
        "Xn = normalize(X)\n",
        "\n",
        "def eval_dbscan(eps_list, ms_list):\n",
        "    rows = []\n",
        "    for eps in eps_list:\n",
        "        for ms in ms_list:\n",
        "            labels = DBSCAN(eps=eps, min_samples=ms, metric=\"cosine\", n_jobs=-1).fit_predict(Xn)\n",
        "            mask = labels != -1\n",
        "            n_noise = int((~mask).sum())\n",
        "            n_keep  = int(mask.sum())\n",
        "            n_clu   = len(np.unique(labels[mask])) if n_keep > 0 else 0\n",
        "            if n_clu >= 2:\n",
        "                sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
        "                dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
        "                ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
        "            else:\n",
        "                sil = dbi = ch = np.nan\n",
        "            rows.append({\n",
        "                \"eps\": float(eps), \"min_samples\": int(ms),\n",
        "                \"n_clusters\": int(n_clu),\n",
        "                \"noise_frac\": n_noise / Xn.shape[0],\n",
        "                \"silhouette_cosine\": sil, \"DBI\": dbi, \"CH\": ch\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df1 = eval_dbscan(EPS_COARSE, MS_LIST)\n",
        "_df1 = df1.copy()\n",
        "_df1[\"_DBI_sort\"] = _df1[\"DBI\"].fillna(np.inf)\n",
        "_df1 = _df1.sort_values(\n",
        "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
        "    ascending=[False, True, False, True]\n",
        ")\n",
        "top1 = _df1.iloc[0]\n",
        "best_eps_coarse = float(top1[\"eps\"])\n",
        "\n",
        "ref_lo = max(0.001, best_eps_coarse - 0.002)\n",
        "ref_hi = best_eps_coarse + 0.002\n",
        "EPS_FINE = np.round(np.arange(ref_lo, ref_hi + 1e-9, 0.001), 3)\n",
        "\n",
        "\n",
        "df2 = eval_dbscan(EPS_FINE, MS_LIST)\n",
        "df_all = pd.concat([df1, df2], ignore_index=True).drop_duplicates(subset=[\"eps\",\"min_samples\"])\n",
        "\n",
        "_df = df_all.copy()\n",
        "_df[\"_DBI_sort\"] = _df[\"DBI\"].fillna(np.inf)\n",
        "_df = _df.sort_values(\n",
        "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
        "    ascending=[False, True, False, True]\n",
        ")\n",
        "\n",
        "best = _df.iloc[0]\n",
        "best_eps = float(best[\"eps\"])\n",
        "best_ms  = int(best[\"min_samples\"])\n",
        "\n",
        "print(_df.drop(columns=[\"_DBI_sort\"]).head(15).to_string(index=False))\n",
        "print(f\"\\n Chọn: eps={best_eps}, min_samples={best_ms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0a235f",
      "metadata": {
        "id": "ad0a235f",
        "outputId": "39ef07d5-9248-4671-85e5-e8b9c5ae58f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eps=0.005 → n_clusters=17, noise=11.2%\n",
            "eps=0.010 → n_clusters=2, noise=0.3%\n",
            "eps=0.015 → n_clusters=1, noise=0.1%\n",
            "eps=0.020 → n_clusters=1, noise=0.0%\n",
            "eps=0.025 → n_clusters=1, noise=0.0%\n",
            "eps=0.030 → n_clusters=1, noise=0.0%\n",
            "eps=0.035 → n_clusters=1, noise=0.0%\n",
            "eps=0.040 → n_clusters=1, noise=0.0%\n",
            "eps=0.045 → n_clusters=1, noise=0.0%\n"
          ]
        }
      ],
      "source": [
        "for eps in np.arange(0.005, 0.05, 0.005):\n",
        "    labels = DBSCAN(eps=eps, min_samples=10, metric='cosine', n_jobs=-1).fit_predict(Xn)\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    noise_frac = (labels == -1).mean()\n",
        "    print(f\"eps={eps:.3f} → n_clusters={n_clusters}, noise={noise_frac:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa7cefa",
      "metadata": {
        "id": "caa7cefa"
      },
      "source": [
        "#### Áp dụng với DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0524fdbc",
      "metadata": {
        "id": "0524fdbc"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "db = DBSCAN(eps=best_eps, min_samples=best_ms, metric=\"cosine\", n_jobs=-1).fit(Xn)\n",
        "labels = db.labels_\n",
        "\n",
        "noise = int((labels == -1).sum())\n",
        "uniq, cnt = np.unique(labels[labels != -1], return_counts=True)\n",
        "print(f\"eps={best_eps}, min_samples={best_ms}\")\n",
        "print(f\"clusters={len(uniq)} | noise={noise} / {len(labels)} ({noise/len(labels):.2%})\")\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbc59ce",
      "metadata": {
        "id": "6fbc59ce"
      },
      "source": [
        "#### đánh giá ( trong trường hợp không biết nhãn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8c72db",
      "metadata": {
        "id": "dd8c72db"
      },
      "outputs": [],
      "source": [
        "# DBSCAN (COSINE) — Cell 3: unsupervised eval + append CSV\n",
        "import pandas as pd, os\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "mask = (labels != -1)\n",
        "if mask.sum() >= 2 and len(np.unique(labels[mask])) >= 2:\n",
        "    sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
        "    dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
        "    ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
        "else:\n",
        "    sil = dbi = ch = float(\"nan\")\n",
        "\n",
        "print(f\"[DBSCAN-cosine | unsupervised] sil={sil:.4f} | DBI={dbi} | CH={ch}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"DBSCAN\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": \"\",\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057ba9e4",
      "metadata": {
        "id": "057ba9e4"
      },
      "source": [
        "#### đánh giá ( trong trường hợp biết nhãn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cffd22b",
      "metadata": {
        "id": "4cffd22b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "df_meta = pd.read_parquet(META_PATH)\n",
        "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
        "ari = float(adjusted_rand_score(y_true, labels))\n",
        "pur = float(purity_score(y_true, labels))\n",
        "\n",
        "print(f\"[DBSCAN-cosine | supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"DBSCAN\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": \"\",\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7371f4ee",
      "metadata": {
        "id": "7371f4ee"
      },
      "source": [
        "### GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc89744e",
      "metadata": {
        "id": "cc89744e"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4e9b9f",
      "metadata": {
        "id": "7e4e9b9f",
        "outputId": "c24e7cc8-eb43-471e-86b5-44978fd6a345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " K  silhouette_cosine\n",
            " 4           0.263736\n",
            "12           0.205098\n",
            "28           0.199713\n",
            "20           0.196263\n",
            "16           0.195344\n",
            "30           0.193094\n",
            "14           0.183494\n",
            " 8           0.179954\n",
            "24           0.179773\n",
            "best_K: 4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "EMB_PATH = \"/Users/huy/Documents/Hutech/HK1A 2025-2026/social-networking/social_networking/pho_bert/outputs_phobert/phobert_embeddings.npy\"\n",
        "Ks = [4, 8, 12, 14, 16, 20, 24, 28, 30]\n",
        "\n",
        "X = np.load(EMB_PATH).astype(np.float64)\n",
        "Xn = normalize(X)\n",
        "\n",
        "rows = []\n",
        "for K in Ks:\n",
        "    gmm = GaussianMixture(n_components=K, covariance_type=\"diag\", reg_covar=1e-4, random_state=42)\n",
        "    labels = gmm.fit_predict(Xn)\n",
        "    sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "    rows.append((K, sil))\n",
        "\n",
        "df_k = pd.DataFrame(rows, columns=[\"K\", \"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
        "best_K = int(df_k.iloc[0][\"K\"])\n",
        "print(df_k.to_string(index=False))\n",
        "print(\"best_K:\", best_K)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bceafbf3",
      "metadata": {
        "id": "bceafbf3"
      },
      "source": [
        "#### Áp dụng với K tìm được"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42052d9",
      "metadata": {
        "id": "a42052d9",
        "outputId": "f14c4889-fdba-44e4-8a47-721353c1c56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_K: 4 | n_clusters: 4\n",
            "cluster 0: 13975\n",
            "cluster 1: 14175\n",
            "cluster 2: 4945\n",
            "cluster 3: 3549\n"
          ]
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "gmm = GaussianMixture(n_components=best_K, covariance_type=\"diag\", reg_covar=1e-4, random_state=42)\n",
        "labels = gmm.fit_predict(Xn)\n",
        "\n",
        "uniq, cnt = np.unique(labels, return_counts=True)\n",
        "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ab31d2",
      "metadata": {
        "id": "89ab31d2"
      },
      "source": [
        "#### đánh giá không nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2b734b",
      "metadata": {
        "id": "5f2b734b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, os\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "dbi = float(davies_bouldin_score(Xn, labels))\n",
        "ch = float(calinski_harabasz_score(Xn, labels))\n",
        "\n",
        "print(f\"[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"GMM\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44d8acf",
      "metadata": {
        "id": "b44d8acf"
      },
      "source": [
        "#### đánh giá khi biết nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379dc052",
      "metadata": {
        "id": "379dc052"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "META_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/outputs_phobert/metadata_phobert.parquet\"\n",
        "EVAL_PATH = \"/home/thinhbq/huy/social_networking/pho_bert/clustering/evaluation_phobert.csv\"\n",
        "\n",
        "df_meta = pd.read_parquet(META_PATH)\n",
        "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
        "ari = float(adjusted_rand_score(y_true, labels))\n",
        "pur = float(purity_score(y_true, labels))\n",
        "\n",
        "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"GMM\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f74a8e",
      "metadata": {
        "id": "c0f74a8e"
      },
      "source": [
        "## multilingual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177fb36e",
      "metadata": {
        "id": "177fb36e",
        "outputId": "459f3f14-20d3-413e-8dd7-444790cb7667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.12.0)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: Pillow in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /Users/huy/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.31.0)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/huy/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
            "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
            "Installing collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [sentence-transformers]ence-transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 safetensors-0.6.2 sentence-transformers-5.1.1 tokenizers-0.22.1 transformers-4.57.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97e4b6f",
      "metadata": {
        "id": "b97e4b6f",
        "outputId": "2054ae11-a5cc-481b-e642-35e96735a330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tf-keras in /Users/huy/Library/Python/3.9/lib/python/site-packages (2.20.1)\n",
            "Requirement already satisfied: tensorflow<2.21,>=2.20 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tf-keras) (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.4.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (58.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.68.1)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.26.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
            "Requirement already satisfied: pillow in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /Users/huy/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /Users/huy/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /Users/huy/Library/Python/3.9/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/huy/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11c94c0",
      "metadata": {
        "id": "d11c94c0"
      },
      "outputs": [],
      "source": [
        "import os, pandas as pd, numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "CSV_PATH = \"/home/thinhbq/huy/social_networking/multilingual/description_multilingual.csv\"\n",
        "OUT_DIR  = \"/home/thinhbq/huy/social_networking/multilingual/outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df[\"description\"] = df[\"description\"].fillna(\"\").astype(str)\n",
        "df = df[df[\"description\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
        "\n",
        "texts = [\"passage: \" + t for t in df[\"description\"].tolist()]\n",
        "\n",
        "emb = model.encode(\n",
        "    texts,\n",
        "    batch_size=16,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "np.save(f\"{OUT_DIR}/embeddings.npy\", emb)\n",
        "df.to_parquet(f\"{OUT_DIR}/metadata.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a7a72b",
      "metadata": {
        "id": "67a7a72b"
      },
      "source": [
        "### Kmean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d6b095",
      "metadata": {
        "id": "17d6b095"
      },
      "source": [
        "#### grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1GcSsxptmG4K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GcSsxptmG4K",
        "outputId": "be5fdc51-2478-4358-ae4e-9ec53186c928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0lN53NpXmnnD",
      "metadata": {
        "id": "0lN53NpXmnnD"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    davies_bouldin_score,\n",
        "    calinski_harabasz_score,\n",
        "    adjusted_rand_score,\n",
        "    normalized_mutual_info_score\n",
        ")\n",
        "from sklearn.metrics.cluster import contingency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0piVR94VnNmY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0piVR94VnNmY",
        "outputId": "673d107d-bc0d-44d4-a8e5-91893577e0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings: (36644, 1024) | Meta: (36644, 2)\n",
            "\n",
            "=== Silhouette (cosine) cho từng K ===\n",
            " K  silhouette_cosine\n",
            " 8           0.182411\n",
            "12           0.178216\n",
            "16           0.165286\n",
            "20           0.150225\n",
            " 4           0.134592\n",
            "24           0.130331\n",
            "30           0.124296\n",
            "28           0.111264\n",
            "40           0.107165\n",
            "50           0.094527\n",
            "\n",
            "=> best_K = 8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\outputs\"\n",
        "EVAL_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\clustering\\evaluation_multilingual.csv\"\n",
        "\n",
        "EMB_PATH  = f\"{OUT_DIR}/embeddings.npy\"\n",
        "META_PATH = f\"{OUT_DIR}/metadata.parquet\"\n",
        "\n",
        "assert Path(EMB_PATH).exists(), f\"Missing: {EMB_PATH}\"\n",
        "assert Path(META_PATH).exists(), f\"Missing: {META_PATH}\"\n",
        "\n",
        "#\n",
        "X = np.load(EMB_PATH)\n",
        "meta = pd.read_parquet(META_PATH)\n",
        "print(\"Embeddings:\", X.shape, \"| Meta:\", meta.shape)\n",
        "Ks = [4, 8, 12, 16, 20, 24, 28, 30, 40, 50]\n",
        "# Chuẩn hóa vector (L2 norm)\n",
        "Xn = normalize(X)\n",
        "from sklearn.utils import parallel_backend\n",
        "with parallel_backend('threading'):\n",
        "#  TÌM K TỐI ƯU\n",
        "    rows = []\n",
        "    for K in Ks:    \n",
        "        # labels = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit_predict(Xn)\n",
        "        labels = KMeans(n_clusters=K, n_init=20, max_iter=500, random_state=42).fit_predict(Xn)\n",
        "        sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "        rows.append((K, sil))\n",
        "\n",
        "df_k = pd.DataFrame(rows, columns=[\"K\",\"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
        "best_K = int(df_k.iloc[0][\"K\"])\n",
        "\n",
        "print(\"\\n=== Silhouette (cosine) cho từng K ===\")\n",
        "print(df_k.to_string(index=False))\n",
        "print(f\"\\n=> best_K = {best_K}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afugxqg4rSt2",
      "metadata": {
        "id": "afugxqg4rSt2"
      },
      "source": [
        "####Áp dụng Kmean với k tìm được"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "MacMF6JknpYC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MacMF6JknpYC",
        "outputId": "ce993e1f-c296-46b9-dfee-1bf2bc09e0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_K: 8 | n_clusters: 8\n",
            "cluster 0: 4923\n",
            "cluster 1: 3871\n",
            "cluster 2: 6558\n",
            "cluster 3: 8893\n",
            "cluster 4: 2716\n",
            "cluster 5: 3264\n",
            "cluster 6: 3007\n",
            "cluster 7: 3412\n"
          ]
        }
      ],
      "source": [
        "km = KMeans(n_clusters=best_K, n_init=\"auto\", random_state=42).fit(Xn)\n",
        "labels = km.labels_\n",
        "\n",
        "uniq, cnt = np.unique(labels, return_counts=True)\n",
        "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RJIjSOx3rfRH",
      "metadata": {
        "id": "RJIjSOx3rfRH"
      },
      "source": [
        "####Đánh giá trong trường hợp không có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ZeiLG3nUnvnE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeiLG3nUnvnE",
        "outputId": "f6dbf227-3a94-4498-a7d2-ddb17007476f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Unsupervised] Silhouette(cosine)=0.1659 | DBI=2.8550 | CH=1471.18\n"
          ]
        }
      ],
      "source": [
        "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "dbi = float(davies_bouldin_score(Xn, labels))\n",
        "ch  = float(calinski_harabasz_score(Xn, labels))\n",
        "\n",
        "print(f\"\\n[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
        "\n",
        "record_unsup = pd.DataFrame([{\n",
        "    \"model\": \"KMeans\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None\n",
        "}])\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record_unsup], ignore_index=True)\n",
        "else:\n",
        "    df_out = record_unsup\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7Rex7tLrlMG",
      "metadata": {
        "id": "o7Rex7tLrlMG"
      },
      "source": [
        "####Đánh giá trong trường hợp có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "OAi1yCP9owrw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAi1yCP9owrw",
        "outputId": "883f28f6-bd49-467e-ec34-6795b2190d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised] NMI=0.8515 | ARI=0.6364 | Purity=0.6579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4128\\1431673235.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record_sup], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "y_true = LabelEncoder().fit_transform(meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
        "ari = float(adjusted_rand_score(y_true, labels))\n",
        "pur = float(purity_score(y_true, labels))\n",
        "\n",
        "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "record_sup = pd.DataFrame([{\n",
        "    \"model\": \"KMeans\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record_sup], ignore_index=True)\n",
        "else:\n",
        "    df_out = record_sup\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7iaGlYfzrI-k",
      "metadata": {
        "id": "7iaGlYfzrI-k"
      },
      "source": [
        "##DBSCAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAr9TNPGrv3N",
      "metadata": {
        "id": "WAr9TNPGrv3N"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "efa781ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eps=0.05 → n_clusters=8\n",
            "eps=0.06 → n_clusters=9\n",
            "eps=0.07 → n_clusters=12\n",
            "eps=0.08 → n_clusters=12\n",
            "eps=0.09 → n_clusters=17\n",
            "eps=0.10 → n_clusters=19\n",
            "eps=0.12 → n_clusters=26\n",
            "eps=0.15 → n_clusters=34\n"
          ]
        }
      ],
      "source": [
        "for eps in [0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.12, 0.15]:\n",
        "    model = DBSCAN(eps=eps, min_samples=15, metric='cosine', n_jobs=-1)\n",
        "    labels = model.fit_predict(Xn)\n",
        "    n_clusters = len(np.unique(labels[labels!=-1]))\n",
        "    print(f\"eps={eps:.2f} → n_clusters={n_clusters}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "-zTMZiUwrLEO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zTMZiUwrLEO",
        "outputId": "ca047607-21e6-4baf-ed1b-afcedbba0eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings: (36644, 1024) | Metadata: (36644, 2)\n",
            " Đã chuẩn hoá vector (L2-norm).\n",
            " eps  min_samples  n_clusters  noise_frac  silhouette_cosine      DBI         CH\n",
            "0.03           20          42    0.926373           0.670061 0.843933 275.124371\n",
            "0.03           15          77    0.909317           0.647748 0.855369 199.528223\n",
            "0.03           18          54    0.919932           0.642889 0.860228 242.839168\n",
            "0.03            5         652    0.784412           0.640021 0.851707  72.921068\n",
            "0.03           12         123    0.891879           0.635623 0.860620 157.278091\n",
            "0.03           10         185    0.872448           0.634774 0.879775 129.697136\n",
            "0.03            8         282    0.846987           0.629001 0.887333 106.659397\n",
            "0.05           20          90    0.801495           0.396389 1.304502 153.590728\n",
            "0.05           18         112    0.784549           0.361638 1.297014 130.459042\n",
            "0.05           15         156    0.756031           0.358383 1.314100 108.672542\n",
            "0.05           12         224    0.721810           0.348244 1.338229  87.869310\n",
            "0.05           10         300    0.685815           0.322910 1.374303  73.578898\n",
            "0.05            8         411    0.646954           0.307069 1.380597  61.406210\n",
            "0.05            5         826    0.556244           0.302370 1.318169  40.229503\n",
            "0.11           12           2    0.020713           0.126438 1.966183   9.207470\n",
            "\n",
            " Chọn: eps=0.03, min_samples=20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "\n",
        "OUT_DIR   = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\outputs\"\n",
        "EMB_PATH  = f\"{OUT_DIR}/embeddings.npy\"\n",
        "META_PATH = f\"{OUT_DIR}/metadata.parquet\"\n",
        "EVAL_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\clustering\\evaluation_multilingual.csv\"\n",
        "\n",
        "\n",
        "assert Path(EMB_PATH).exists(), f\"Không tìm thấy file: {EMB_PATH}\"\n",
        "assert Path(META_PATH).exists(), f\"Không tìm thấy file: {META_PATH}\"\n",
        "\n",
        "X = np.load(EMB_PATH)\n",
        "meta = pd.read_parquet(META_PATH)\n",
        "\n",
        "print(\"Embeddings:\", X.shape, \"| Metadata:\", meta.shape)\n",
        "\n",
        "\n",
        "Xn = normalize(X, copy=False)\n",
        "print(\" Đã chuẩn hoá vector (L2-norm).\")\n",
        "\n",
        "# EPS_COARSE = [0.003, 0.005, 0.007, 0.009, 0.011, 0.013]\n",
        "EPS_COARSE = np.round(np.arange(0.03, 0.13, 0.02), 3)   # từ 0.03 → 0.11\n",
        "MS_LIST    = [5, 8, 10, 12, 15, 18, 20]\n",
        "\n",
        "def eval_dbscan(eps_list, ms_list):\n",
        "    rows = []\n",
        "    for eps in eps_list:\n",
        "        for ms in ms_list:\n",
        "            labels = DBSCAN(eps=eps, min_samples=ms, metric=\"cosine\", n_jobs=-1).fit_predict(Xn)\n",
        "            mask = (labels != -1)\n",
        "            n_noise = int((~mask).sum())\n",
        "            n_keep  = int(mask.sum())\n",
        "            n_clu   = len(np.unique(labels[mask])) if n_keep > 0 else 0\n",
        "\n",
        "            if n_clu >= 2:\n",
        "                sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
        "                dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
        "                ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
        "            else:\n",
        "                sil = dbi = ch = np.nan\n",
        "\n",
        "            rows.append({\n",
        "                \"eps\": float(eps),\n",
        "                \"min_samples\": int(ms),\n",
        "                \"n_clusters\": int(n_clu),\n",
        "                \"noise_frac\": n_noise / len(Xn),\n",
        "                \"silhouette_cosine\": sil,\n",
        "                \"DBI\": dbi,\n",
        "                \"CH\": ch\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "df1 = eval_dbscan(EPS_COARSE, MS_LIST)\n",
        "_df1 = df1.copy()\n",
        "_df1[\"_DBI_sort\"] = _df1[\"DBI\"].fillna(np.inf)\n",
        "_df1 = _df1.sort_values(\n",
        "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
        "    ascending=[False, True, False, True]\n",
        ")\n",
        "\n",
        "\n",
        "top1 = _df1.iloc[0]\n",
        "best_eps_coarse = float(top1[\"eps\"])\n",
        "\n",
        "# print(\"Kết quả COARSE SEARCH:\")\n",
        "# print(_df1.drop(columns=[\"_DBI_sort\"]).head(10).to_string(index=False))\n",
        "# print(f\"\\n best_eps_coarse = {best_eps_coarse}\")\n",
        "\n",
        "# ref_lo = max(0.001, best_eps_coarse - 0.002)\n",
        "# ref_hi = best_eps_coarse + 0.002\n",
        "# EPS_FINE = np.round(np.arange(ref_lo, ref_hi + 1e-9, 0.001), 3)\n",
        "\n",
        "ref_lo = max(0.05, best_eps_coarse - 0.02)\n",
        "ref_hi = best_eps_coarse + 0.02\n",
        "EPS_FINE = np.round(np.arange(ref_lo, ref_hi + 1e-9, 0.002), 3)\n",
        "\n",
        "\n",
        "df2 = eval_dbscan(EPS_FINE, MS_LIST)\n",
        "df_all = pd.concat([df1, df2], ignore_index=True).drop_duplicates(subset=[\"eps\",\"min_samples\"])\n",
        "\n",
        "_df = df_all.copy()\n",
        "_df[\"_DBI_sort\"] = _df[\"DBI\"].fillna(np.inf)\n",
        "_df = _df.sort_values(\n",
        "    by=[\"silhouette_cosine\",\"_DBI_sort\",\"CH\",\"noise_frac\"],\n",
        "    ascending=[False, True, False, True]\n",
        ")\n",
        "\n",
        "best = _df.iloc[0]\n",
        "best_eps = float(best[\"eps\"])\n",
        "best_ms  = int(best[\"min_samples\"])\n",
        "\n",
        "print(_df.drop(columns=[\"_DBI_sort\"]).head(15).to_string(index=False))\n",
        "print(f\"\\n Chọn: eps={best_eps}, min_samples={best_ms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "76333e60",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eps=0.030 → n_clusters=185, noise=87.2%\n",
            "eps=0.040 → n_clusters=260, noise=80.3%\n",
            "eps=0.050 → n_clusters=300, noise=68.6%\n",
            "eps=0.060 → n_clusters=258, noise=52.7%\n",
            "eps=0.070 → n_clusters=173, noise=35.5%\n",
            "eps=0.080 → n_clusters=75, noise=20.8%\n",
            "eps=0.090 → n_clusters=23, noise=10.5%\n",
            "eps=0.100 → n_clusters=3, noise=4.8%\n",
            "eps=0.110 → n_clusters=2, noise=1.9%\n",
            "eps=0.120 → n_clusters=1, noise=0.7%\n"
          ]
        }
      ],
      "source": [
        "for eps in np.arange(0.03, 0.13, 0.01):\n",
        "    labels = DBSCAN(eps=eps, min_samples=10, metric='cosine', n_jobs=-1).fit_predict(Xn)\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    noise_frac = (labels == -1).mean()\n",
        "    print(f\"eps={eps:.3f} → n_clusters={n_clusters}, noise={noise_frac:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ibtdF8ry7n",
      "metadata": {
        "id": "23ibtdF8ry7n"
      },
      "source": [
        "#### Áp dụng với DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ilvz2-ogrzS4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilvz2-ogrzS4",
        "outputId": "ca4f6199-fc65-4e63-8584-6823feabbebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eps=0.03, min_samples=20\n",
            "clusters=42 | noise=33946/36644 (92.64%)\n",
            "cluster 0: 22\n",
            "cluster 1: 62\n",
            "cluster 2: 78\n",
            "cluster 3: 20\n",
            "cluster 4: 20\n",
            "cluster 5: 71\n",
            "cluster 6: 67\n",
            "cluster 7: 56\n",
            "cluster 8: 35\n",
            "cluster 9: 21\n",
            "cluster 10: 340\n",
            "cluster 11: 32\n",
            "cluster 12: 20\n",
            "cluster 13: 108\n",
            "cluster 14: 32\n",
            "cluster 15: 63\n",
            "cluster 16: 20\n",
            "cluster 17: 52\n",
            "cluster 18: 187\n",
            "cluster 19: 63\n",
            "cluster 20: 20\n",
            "cluster 21: 26\n",
            "cluster 22: 31\n",
            "cluster 23: 52\n",
            "cluster 24: 34\n",
            "cluster 25: 27\n",
            "cluster 26: 20\n",
            "cluster 27: 22\n",
            "cluster 28: 22\n",
            "cluster 29: 77\n",
            "cluster 30: 50\n",
            "cluster 31: 44\n",
            "cluster 32: 38\n",
            "cluster 33: 36\n",
            "cluster 34: 67\n",
            "cluster 35: 25\n",
            "cluster 36: 22\n",
            "cluster 37: 34\n",
            "cluster 38: 35\n",
            "cluster 39: 487\n",
            "cluster 40: 71\n",
            "cluster 41: 89\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "db = DBSCAN(eps=best_eps, min_samples=best_ms, metric=\"cosine\", n_jobs=-1).fit(Xn)\n",
        "labels = db.labels_\n",
        "\n",
        "noise = int((labels == -1).sum())\n",
        "uniq, cnt = np.unique(labels[labels != -1], return_counts=True)\n",
        "\n",
        "print(f\"eps={best_eps}, min_samples={best_ms}\")\n",
        "print(f\"clusters={len(uniq)} | noise={noise}/{len(labels)} ({noise/len(labels):.2%})\")\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-R7liBkr1ot",
      "metadata": {
        "id": "a-R7liBkr1ot"
      },
      "source": [
        "#### Đánh giá ( trong trường hợp không biết nhãn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "UgxJEVjPr172",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgxJEVjPr172",
        "outputId": "52807fb1-8c12-472b-b803-7a7ecc24d8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DBSCAN-cosine | unsupervised] Silhouette=0.6701 | DBI=0.8439 | CH=275.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4128\\893157196.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record_unsup], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "\n",
        "mask = (labels != -1)\n",
        "if mask.sum() >= 2 and len(np.unique(labels[mask])) >= 2:\n",
        "    sil = float(silhouette_score(Xn[mask], labels[mask], metric=\"cosine\"))\n",
        "    dbi = float(davies_bouldin_score(Xn[mask], labels[mask]))\n",
        "    ch  = float(calinski_harabasz_score(Xn[mask], labels[mask]))\n",
        "else:\n",
        "    sil = dbi = ch = np.nan\n",
        "\n",
        "print(f\"[DBSCAN-cosine | unsupervised] Silhouette={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
        "\n",
        "record_unsup = pd.DataFrame([{\n",
        "    \"model\": \"DBSCAN\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": len(uniq),\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None}])\n",
        "\n",
        "os.makedirs(os.path.dirname(EVAL_PATH), exist_ok=True)\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record_unsup], ignore_index=True)\n",
        "else:\n",
        "    df_out = record_unsup\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ivxLwxH2r2Ob",
      "metadata": {
        "id": "ivxLwxH2r2Ob"
      },
      "source": [
        "#### Đánh giá ( trong trường hợp biết nhãn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "lkrllOTYr5e-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkrllOTYr5e-",
        "outputId": "7647e94b-cf94-4986-edc8-f38b0105c77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DBSCAN-cosine | supervised] NMI=0.7577 | ARI=0.5280 | Purity=1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4128\\1238425811.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record_sup], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR   = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\outputs\"\n",
        "EMB_PATH  = f\"{OUT_DIR}/embeddings.npy\"\n",
        "META_PATH = f\"{OUT_DIR}/metadata.parquet\"\n",
        "EVAL_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\clustering\\evaluation_multilingual.csv\"\n",
        "meta = pd.read_parquet(META_PATH)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "import numpy as np\n",
        "\n",
        "y_true = LabelEncoder().fit_transform(meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "mask = (labels != -1)\n",
        "y_true_f = y_true[mask]\n",
        "y_pred_f = labels[mask]\n",
        "\n",
        "if len(np.unique(y_pred_f)) >= 2:\n",
        "    nmi = float(normalized_mutual_info_score(y_true_f, y_pred_f, average_method=\"arithmetic\"))\n",
        "    ari = float(adjusted_rand_score(y_true_f, y_pred_f))\n",
        "    pur = float(purity_score(y_true_f, y_pred_f))\n",
        "else:\n",
        "    nmi = ari = pur = np.nan\n",
        "\n",
        "print(f\"[DBSCAN-cosine | supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "# ========== GHI CSV ==========\n",
        "record_sup = pd.DataFrame([{\n",
        "    \"model\": \"DBSCAN\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": len(np.unique(labels[labels != -1])),\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record_sup], ignore_index=True)\n",
        "else:\n",
        "    df_out = record_sup\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PCN9B-9xyfX9",
      "metadata": {
        "id": "PCN9B-9xyfX9"
      },
      "source": [
        "###GMM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fz-UVIlkyim6",
      "metadata": {
        "id": "fz-UVIlkyim6"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "PQhK7Xj-yhBV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQhK7Xj-yhBV",
        "outputId": "feda9c52-63d7-4e6b-c976-6d705a8ec45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " K  silhouette_cosine\n",
            " 8           0.165806\n",
            "14           0.156887\n",
            "12           0.150320\n",
            "16           0.141044\n",
            " 4           0.129420\n",
            "20           0.122766\n",
            "24           0.116940\n",
            "28           0.115119\n",
            "30           0.115060\n",
            "best_K: 8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "EMB_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\outputs\\embeddings.npy\"\n",
        "Ks = [4, 8, 12, 14, 16, 20, 24, 28, 30]\n",
        "\n",
        "\n",
        "X = np.load(EMB_PATH).astype(np.float64)\n",
        "Xn = normalize(X)\n",
        "\n",
        "\n",
        "rows = []\n",
        "for K in Ks:\n",
        "    gmm = GaussianMixture(\n",
        "        n_components=K,\n",
        "        covariance_type=\"diag\",\n",
        "        reg_covar=1e-4,\n",
        "        random_state=42\n",
        "    )\n",
        "    labels = gmm.fit_predict(Xn)\n",
        "    sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "    rows.append((K, sil))\n",
        "\n",
        "df_k = pd.DataFrame(rows, columns=[\"K\", \"silhouette_cosine\"]).sort_values(\"silhouette_cosine\", ascending=False)\n",
        "best_K = int(df_k.iloc[0][\"K\"])\n",
        "\n",
        "print(df_k.to_string(index=False))\n",
        "print(\"best_K:\", best_K)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T-pBs_rxyjTO",
      "metadata": {
        "id": "T-pBs_rxyjTO"
      },
      "source": [
        "#### Áp dụng với K tìm được"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "zL9GVyt-yoTd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL9GVyt-yoTd",
        "outputId": "8448295d-8202-45f1-8fe8-fa5fbd63fc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_K: 8 | n_clusters: 8\n",
            "cluster 0: 4925\n",
            "cluster 1: 3872\n",
            "cluster 2: 6515\n",
            "cluster 3: 8834\n",
            "cluster 4: 2766\n",
            "cluster 5: 3245\n",
            "cluster 6: 3079\n",
            "cluster 7: 3408\n"
          ]
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "# ========== GMM CLUSTERING (APPLY BEST K) ==========\n",
        "gmm = GaussianMixture(\n",
        "    n_components=best_K,covariance_type=\"diag\", reg_covar=1e-4, random_state=42)\n",
        "labels = gmm.fit_predict(Xn)\n",
        "\n",
        "uniq, cnt = np.unique(labels, return_counts=True)\n",
        "\n",
        "print(\"best_K:\", best_K, \"| n_clusters:\", len(uniq))\n",
        "for k, c in zip(uniq, cnt):\n",
        "    print(f\"cluster {k}: {c}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WhFMZCW8yoov",
      "metadata": {
        "id": "WhFMZCW8yoov"
      },
      "source": [
        "#### Đánh giá cho trường hợp không có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "z1ojtu7Jyx93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1ojtu7Jyx93",
        "outputId": "e0916785-c81d-4a16-b259-d346a3d5b183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Unsupervised] Silhouette(cosine)=0.1658 | DBI=2.8624 | CH=1470.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4128\\3699063189.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, os\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "\n",
        "EVAL_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\clustering\\evaluation_multilingual.csv\"\n",
        "\n",
        "\n",
        "sil = float(silhouette_score(Xn, labels, metric=\"cosine\"))\n",
        "dbi = float(davies_bouldin_score(Xn, labels))\n",
        "ch  = float(calinski_harabasz_score(Xn, labels))\n",
        "\n",
        "print(f\"[Unsupervised] Silhouette(cosine)={sil:.4f} | DBI={dbi:.4f} | CH={ch:.2f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"GMM\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"unsupervised\",\n",
        "    \"silhouette_cosine\": sil,\n",
        "    \"DBI\": dbi,\n",
        "    \"CH\": ch,\n",
        "    \"NMI\": None,\n",
        "    \"ARI\": None,\n",
        "    \"Purity\": None\n",
        "}])\n",
        "\n",
        "\n",
        "os.makedirs(os.path.dirname(EVAL_PATH), exist_ok=True)\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eR8kzWXsyywh",
      "metadata": {
        "id": "eR8kzWXsyywh"
      },
      "source": [
        "#### Đánh giá cho trường họp có nhãn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "OSnhtd6wy1nF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSnhtd6wy1nF",
        "outputId": "cfd40e3f-f0e6-4b6d-feef-70f329cb5740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised] NMI=0.8516 | ARI=0.6388 | Purity=0.6591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4128\\533143123.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_out = pd.concat([old, record], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "META_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\outputs\\metadata.parquet\"\n",
        "EVAL_PATH = r\"D:\\Mang_XH\\git_mang_xh\\social-networking\\social_networking\\multilingual\\clustering\\evaluation_multilingual.csv\"\n",
        "\n",
        "df_meta = pd.read_parquet(META_PATH)\n",
        "y_true = LabelEncoder().fit_transform(df_meta[\"category_name\"].astype(str).values)\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    cm = contingency_matrix(y_true, y_pred)\n",
        "    return cm.max(axis=0).sum() / cm.sum()\n",
        "\n",
        "nmi = float(normalized_mutual_info_score(y_true, labels, average_method=\"arithmetic\"))\n",
        "ari = float(adjusted_rand_score(y_true, labels))\n",
        "pur = float(purity_score(y_true, labels))\n",
        "\n",
        "print(f\"[Supervised] NMI={nmi:.4f} | ARI={ari:.4f} | Purity={pur:.4f}\")\n",
        "\n",
        "record = pd.DataFrame([{\n",
        "    \"model\": \"GMM\",\n",
        "    \"metric\": \"cosine\",\n",
        "    \"K\": best_K,\n",
        "    \"type\": \"supervised\",\n",
        "    \"silhouette_cosine\": None,\n",
        "    \"DBI\": None,\n",
        "    \"CH\": None,\n",
        "    \"NMI\": nmi,\n",
        "    \"ARI\": ari,\n",
        "    \"Purity\": pur\n",
        "}])\n",
        "\n",
        "os.makedirs(os.path.dirname(EVAL_PATH), exist_ok=True)\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    old = pd.read_csv(EVAL_PATH)\n",
        "    df_out = pd.concat([old, record], ignore_index=True)\n",
        "else:\n",
        "    df_out = record\n",
        "\n",
        "df_out.to_csv(EVAL_PATH, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
